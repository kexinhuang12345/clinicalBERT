{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: load Note datasets\n",
    "df_notes = pd.read_csv('NOTE_DATA_PATH/NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Generate the cohort to pretrain on (here e.g. we only pretrian on physician and nursing notes) \n",
    "df_notes = df_notes[df_notes.CATEGORY.isin(['Physician ','Nursing','Nursing/Others'])]\n",
    "\n",
    "# IMPORTANT: if you fine tune on the same dataset that you use for pretrain, you need to preclude the fine-tune test admissions \n",
    "df_test_ids = pd.read_csv('FINETUNE_DATA_PATH/test.csv').HADM_ID.unique()\n",
    "df_notes_fold = df_notes[~df_notes.HADM_ID.isin(df_test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Preprocessing\n",
    "def preprocess1(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    \n",
    "    # remove, digits, spaces\n",
    "    y = y.translate(str.maketrans(\"\", \"\", string.digits))\n",
    "    y = \" \".join(y.split())\n",
    "    return y\n",
    "\n",
    "def preprocessing(df_notes): \n",
    "    df_notes['TEXT']=df_notes['TEXT'].fillna(' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\n',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\r',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(str.strip)\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.lower()\n",
    "\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(lambda x: preprocess1(x))\n",
    "    \n",
    "    return df_notes\n",
    "\n",
    "df_notes_fold = preprocessing(df_notes_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Notes to Sentences\n",
    "from spacy.lang.en import English\n",
    "nlp = English()  # just the language with no model\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "# nlp praser may not work when there is only one token. In these cases, we just remove them as note that has length 1 usually is some random stuff\n",
    "\n",
    "def toSentence(x):\n",
    "    doc = nlp(x)\n",
    "    text=[]\n",
    "    try:\n",
    "        for sent in doc.sents:\n",
    "            st=str(sent).strip() \n",
    "            if len(st)<20:\n",
    "                #a lot of abbreviation is segmented as one line. But these are all describing the previous things\n",
    "                #so I attached it to the sentence before\n",
    "                if len(text)!=0:\n",
    "                    text[-1]=' '.join((text[-1],st))\n",
    "                else:\n",
    "                    text=[st]\n",
    "            else:\n",
    "                text.append((st))\n",
    "    except:\n",
    "        print(doc)\n",
    "    return text\n",
    "\n",
    "pretrain_sent=df_notes_fold['TEXT'].apply(lambda x: toSentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Create Pretraining File\n",
    "file=open('PRETRAIN_DATA_PATH/clinical_sentences_pretrain.txt','w')\n",
    "pretrain_sent = pretrain_sent.values\n",
    "for i in tqdm(range(len(pretrain_sent))):\n",
    "    if len(pretrain_sent[i]) > 0:\n",
    "        # remove the one token note\n",
    "        note = pretrain_sent[i]\n",
    "        for sent in note:\n",
    "            file.write(sent+'\\n')\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Download the Implementations and Initial Checkpoint with the SentencePiece Model and Vocab \n",
    "# Github Repo:\n",
    "# XLNet: git clone https://github.com/zihangdai/xlnet.git\n",
    "# BERT: git clone https://github.com/google-research/bert.git\n",
    "# Model: \n",
    "# XLNet-Base: https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip\n",
    "# BERT-Base-Uncased: https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Generate Pretraining Tensorflow TF_Records\n",
    "\n",
    "# For Clinical BERT\n",
    "# cd to the git repo\n",
    "\n",
    "# Generate datasets for 128 max seq\n",
    "!python create_pretraining_data.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/clinical_sentences_pretrain.txt \\\n",
    "  --output_file=PRETRAIN_DATA_PATH/tf_examples_128.tfrecord \\\n",
    "  --vocab_file=INITIAL_MODEL_PATH/vocab.txt \\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --random_seed=12345 \\\n",
    "  --dupe_factor=3\n",
    "\n",
    "# Generate datasets for 512 max seq\n",
    "!python create_pretraining_data.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/clinical_sentences_pretrain.txt \\\n",
    "  --output_file=PRETRAIN_DATA_PATH/tf_examples_512.tfrecord \\\n",
    "  --vocab_file=INITIAL_MODEL_PATH/vocab.txt \\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=512 \\\n",
    "  --max_predictions_per_seq=76 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --random_seed=12345 \\\n",
    "  --dupe_factor=3\n",
    "\n",
    "# For Clinical XLNet\n",
    "\n",
    "!python data_utils.py \\\n",
    "    --bsz_per_host=6 \\\n",
    "    --num_core_per_host=1 \\\n",
    "    --seq_len=512 \\\n",
    "    --reuse_len=256 \\\n",
    "    --input_glob=/scratch/kh2383/MechVent/data/clinical_sentences_pretrain_xlnet.txt \\\n",
    "    --save_dir=/scratch/kh2383/MechVent/data/xlnet_tfrecord/ \\\n",
    "    --num_passes=5 \\\n",
    "    --bi_data=True \\\n",
    "    --sp_path=/scratch/kh2383/clibert/xlnet_cased_L-12_H-768_A-12/spiece.model \\\n",
    "    --mask_alpha=6 \\\n",
    "    --mask_beta=1 \\\n",
    "    --num_predict=85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Pretraining Use Original TF implementation\n",
    "\n",
    "# For Clinical BERT\n",
    "\n",
    "# First pretrain 100000 steps on the max seq length of 128\n",
    "!python run_pretraining.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/tf_examples_128.tfrecord \\\n",
    "  --output_dir=PRETRAINED_MODEL_PATH/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=INITIAL_DATA_PATH/bert_config.json \\\n",
    "  --init_checkpoint=INITIAL_DATA_PATH/bert_model.ckpt \\\n",
    "  --train_batch_size=64 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --num_train_steps=100000 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5\n",
    "\n",
    "# Then further pretrain 100000 steps on the max seq length of 512\n",
    "# NOTE: the init_checkpoint should switch to the 128 pretrained model\n",
    "\n",
    "!python run_pretraining.py \\\n",
    "  --input_file=PRETRAIN_DATA_PATH/tf_examples_512.tfrecord \\\n",
    "  --output_dir=PRETRAINED_MODEL_PATH/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=INITIAL_DATA_PATH/bert_config.json \\\n",
    "  --init_checkpoint=PRETRAINED_MODEL_PATH/pretraining_output_128/model.ckpt-100000 \\\n",
    "  --train_batch_size=16 \\\n",
    "  --max_seq_length=512 \\\n",
    "  --max_predictions_per_seq=76 \\\n",
    "  --num_train_steps=100000 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5\n",
    "\n",
    "\n",
    "# For Clinical XLNet\n",
    "\n",
    "# Pretrain for 200000 steps\n",
    "\n",
    "!python train_gpu.py \\\n",
    "    --record_info_dir=PRETRAIN_DATA_PATH/xlnet_tfrecord/tfrecords/ \\\n",
    "    --model_dir=PRETRAINED_MODEL_PATH/xlnet_model/ \\\n",
    "    --init_checkpoint=INITIAL_DATA_PATH/xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt \\\n",
    "    --train_batch_size=8 \\\n",
    "    --seq_len=512 \\\n",
    "    --reuse_len=256 \\\n",
    "    --mem_len=384 \\\n",
    "    --perm_size=256 \\\n",
    "    --n_layer=12 \\\n",
    "    --d_model=768 \\\n",
    "    --d_embed=768 \\\n",
    "    --n_head=12 \\\n",
    "    --d_head=64 \\\n",
    "    --d_inner=3072 \\\n",
    "    --untie_r=True \\\n",
    "    --mask_alpha=6 \\\n",
    "    --mask_beta=1 \\\n",
    "    --num_predict=85 \\\n",
    "    --num_hosts=1 \\\n",
    "    --num_core_per_host=2 \\\n",
    "    --train_steps=200000 \\\n",
    "    --iterations=500 \\\n",
    "    --save_steps=5000 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
